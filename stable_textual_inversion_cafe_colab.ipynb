{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rRpa4FDV93tf"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/X3n0nP0w3r3d/yepTest/blob/master/stable_textual_inversion_cafe_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cafe's TI repo Colab implemented by **andite**.\n",
        "\n",
        "Textual Inversion repo from **Cafe**. Thank you for this wonderful TI repo, all credits goes to him. :)\n",
        "\n",
        "https://github.com/cafeai/stable-textual-inversion-cafe\n",
        "\n",
        "<fieldset>\n",
        "\n",
        "Discord:\n",
        "\n",
        "Cafe - `Starport ‚Äî „Åã„Åµ„Åá#0438`\n",
        "\n",
        "andite - `andite#8484`\n",
        "</fieldset>\n",
        "\n",
        "Please contact me through discord if you found any errors in this repo or colab.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TXAVZ_66-D0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Update [12/8/22]**\n",
        "\n",
        "\n",
        "\n",
        "*   Added the latest pytorch version to fix the training cell. It works now. <font color= grey> Credits to legekka#4242 for helping me out.\n",
        "\n",
        "    <image src=https://media.tenor.com/o-0LaJK3qWcAAAAC/yamada-ryou-yamada-ryo.gif>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_rphctpXXaYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to use this colab?**\n",
        "\n",
        "Requirements:\n",
        "\n",
        ">Common sense\n",
        "\n",
        ">Ability to read\n",
        "\n",
        "Why not train in auto's webui instead though? ü§ìü§ì\n",
        "\n",
        ">The training from the webui is really bad compared to the original TI code(well, for now), it's slightly dysfunctional resulting to a somehow messier result.\n",
        "\n",
        ">This one uses 225 tokens per step, which means it's **6x** much more faster compared to training in webui. 1000 steps can be considered as **6000 steps** in this repo. `1 step * 6`\n",
        "\n",
        "W-wait, what's a t-textual inversion? üêí\n",
        "\n",
        "> https://textual-inversion.github.io\n",
        "\n"
      ],
      "metadata": {
        "id": "J97xvTGMFfcH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8m4ylgx3vS2",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5127ed2-ced2-43b8-abea-8635c185fb3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 28 06:05:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "GPU 0: Tesla T4 (UUID: GPU-0aac2ba0-1ee8-88cb-41b9-cee4d9bf19bb)\n"
          ]
        }
      ],
      "source": [
        "#@title Check your GPU.\n",
        "# restart runtime until it gets better gpu\n",
        "from IPython.display import HTML\n",
        "from subprocess import getoutput\n",
        "s = getoutput('nvidia-smi')\n",
        "print(s)\n",
        "# or simply\n",
        "!nvidia-smi -L\n",
        "!pip install -q -U gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount your google drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cd /content/drive/MyDrive/\n",
        "!mkdir /content/drive/MyDrive/sd_text_inversion\n",
        "!mkdir -p /content/drive/MyDrive/sd_text_inversion/Imagesfortraining"
      ],
      "metadata": {
        "id": "4TN99ds13_4Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grab your model.\n",
        "#@markdown Use a huggingface or drive link for the model you'll be using in training.\n",
        "!pip install -q -U gdown\n",
        "#@markdown <font color = \"yellow\">Note: For some reason, Anything model isn't compatible with training so I advise you not to use it.\n",
        "url = \"https://huggingface.co/andite/models/resolve/main/nai-wd.ckpt\" #@param {type:\"string\"}\n",
        "!gdown {url} -O /content/drive/MyDrive/sd_text_inversion/"
      ],
      "metadata": {
        "id": "TPXs-8WZ_QEm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put the training images inside `/content/drive/MyDrive/sd_text_inversion/Imagesfortraining` in there, make sure the files inside are only images. You don't need deepbooru or blip captions for your images since TI only trains in visuals.\n",
        "\n",
        ">**IMPORTANT: Make sure your training images has 512x512 resolution. You could preprocess them through the webui if you don't wanna do it manually**\n",
        "\n",
        "><font color=yellow>Note: You don't need to mirror your datasets, this repo does it automatically for you. Doing so would just overcook your training.</font>\n",
        "\n",
        "You could use the cell below if you want to use a link instead. It should be a zip file. "
      ],
      "metadata": {
        "id": "9cfLg4Aw53Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get your training images/datasets.\n",
        "#@markdown Make sure to use a huggingface or drive link as well.\n",
        "url = \"https://drive.google.com/file/uc?id=1QGVRe01x8AYM3slPkGpIhjf5gs7-VwOW\" #@param {type:\"string\"}\n",
        "name = \"yohandataset\" #@param {type:\"string\"}\n",
        "!gdown {url} -O /content/{name}.zip\n",
        "!unzip /content/{name}.zip -d /content/drive/MyDrive/sd_text_inversion/Imagesfortraining/{name}"
      ],
      "metadata": {
        "id": "FpIBFdDc_hDR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get the repo for textual inversion.\n",
        "#@markdown *Note: No need to run this if you already have the repo inside your drive, but if it's your first time using this colab in your email, run the cell.*\n",
        "%cd /content/drive/MyDrive/sd_text_inversion\n",
        "!mkdir modelsbackup\n",
        "!mkdir /content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe\n",
        "!mkdir /content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe/logs/\n",
        "!mv /content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe/logs/ /content/drive/MyDrive/sd_text_inversion/modelsbackup/\n",
        "!rm -rf /content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe\n",
        "%cd /content/drive/MyDrive/sd_text_inversion\n",
        "!git clone https://github.com/Raearn/stable-textual-inversion-cafe.git\n",
        "%cd /content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MDttzbd74sh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install python packages. \n",
        "#@markdown *It is highly required to install the packages or else the training won't work.*\n",
        "%cd /content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe\n",
        "!pip install omegaconf einops pytorch-lightning==1.6.5 test-tube transformers kornia -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!pip install setuptools==59.5.0\n",
        "!pip install pillow==9.0.1\n",
        "!pip install torchmetrics==0.6.0\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install torchtext==0.13.1\n",
        "!pip install -e .\n",
        "print(\"Done. run the cell under this to enable all packages\")"
      ],
      "metadata": {
        "id": "rDCcZcop4pUa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this cell after installing in order to activate the package. It will crash the runtime but that's how it's supposed to be.\n",
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "id": "O3s7tfnn40AE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Edit the yaml file first based from the config you will choose in the cell before training!\n",
        "\n",
        "You can click the directory link below instead of having the trouble of skimming through your folders\n",
        "\n",
        "> artstyle - `/content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe/configs/stable-diffusion/artstyle.yaml`\n",
        "\n",
        "> character - `/content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe/configs/stable-diffusion/character.yaml`\n",
        "\n",
        "\n",
        "In line 29, set your vector tokens depending on the amount of training images you have.\n",
        "\n",
        ">`45+ images - 8 tokens`\n",
        "\n",
        ">`100-150+ images - 16 tokens`\n",
        "\n",
        ">`300++ images - 20 or more tokens`\n",
        "\n",
        "Optional: In line 27, you can add some additional initializer words inside the square brackets if you want the training to focus more or make a certain part more accurate. (I am not talking about the initializer_word inside the train cell.)\n",
        "\n",
        ">For example, put `\"hair\"` inside the square brackets if you want it to focus more on the accuracy for the hair especially if it's complex or unique. You could also put words like `\"painting\"` or `\"lighting\"`.\n",
        "\n",
        "Optional: You can set a custom learning rate in line 2, but `5.0e-03` / `0.005` is the default and the safest value.\n",
        "\n",
        "Training preview images and checkpoints/pt files should be in `/content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe/logs`\n",
        "\n",
        "> Note: The actual training preview images are named `samples_scaled_gs` from the logs, which means, `inputs_gs`, `reconstruction_gs`, `samples_gs`,` aren't the actual **training preview images**. Ignore them."
      ],
      "metadata": {
        "id": "h8poJvsiAacq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train.\n",
        "artstyle = \"configs/stable-diffusion/artstyle.yaml\"\n",
        "character = \"configs/stable-diffusion/character.yaml\"\n",
        "config = artstyle #@param [\"artstyle\", \"character\"] {type:\"raw\"}\n",
        "model = \"/content/drive/MyDrive/sd_text_inversion/nai-wd.ckpt\" #@param {type:\"string\"}\n",
        "dataset = \"/content/drive/MyDrive/sd_text_inversion/Imagesfortraining/yohandataset/yohan\" #@param {type:\"string\"}\n",
        "project_name = \"yohanTI\" #@param {type:\"string\"}\n",
        "initializer_word = \"illustration\" #@param {type:\"string\"}\n",
        "%cd /content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe\n",
        "!python \"main.py\" \\\n",
        " --base {config} \\\n",
        " -t --no-test \\\n",
        " --actual_resume {model} \\\n",
        " -n {project_name} \\\n",
        " --gpus 1 \\\n",
        " --data_root {dataset} \\\n",
        " --init_word {initializer_word}"
      ],
      "metadata": {
        "id": "74Q3sWK745sW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set initializer_word to `illustration` for artstyle, or `character` for character. Warning, initializer_word isn't supposed to be the file name for your embed.\n",
        "\n",
        "You can use a custom word though if you'd like.\n",
        "\n",
        "Note: This trains forever, so you could just stop the cell from running and it would still save the current embedding if you're satisfied from the results already. \n",
        "\n",
        "> <font color = 'red'>**DO NOT OVERTRAIN.** The amount of steps you need also depends on the amount of datasets you have. 10k steps and below are usually fine for this repo, but you also have to see for yourself by testing your embeds. Do alot of comparisons with grids.\n",
        "\n",
        "> For characters, you only need around 3-5k steps depending on its complexity. However, you'll need alot more for artstyles. I can't say an estimate since it really depends on what kind of artstyle you're trying to train. The more complex it is, the more steps you need. Lowering the learning rate is also recommended so it wouldn't skip some details and have a less chance of loss rate.\n",
        "</font>"
      ],
      "metadata": {
        "id": "5PB-2A5eEcoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Resume training here.**"
      ],
      "metadata": {
        "id": "rRpa4FDV93tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Retrain.\n",
        "#@markdown Install python packages first before retraining or else it won't work!\n",
        "\n",
        "#@markdown Don't forget to edit your config file, should be the same setting as the one you did for the TI you trained.\n",
        "%cd /content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe\n",
        "artstyle = \"configs/stable-diffusion/artstyle.yaml\"\n",
        "character = \"configs/stable-diffusion/character.yaml\"\n",
        "config = artstyle #@param [\"artstyle\", \"character\"] {type:\"raw\"}\n",
        "model = \"/content/drive/MyDrive/sd_text_inversion/wd-v1-3-float32.ckpt\" #@param {type:\"string\"}\n",
        "last_embed = \"/content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe/logs/yohan2022-11-12T12-52-32_yohan/checkpoints/embeddings_gs-500.pt\" #@param {type:\"string\"}\n",
        "last_checkpoint = \"/content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe/logs/yohan2022-11-12T12-52-32_yohan/checkpoints/last.ckpt\" #@param {type:\"string\"}\n",
        "dataset = \"/content/drive/MyDrive/sd_text_inversion/Imagesfortraining/yohandataset/yohan\" #@param {type:\"string\"}\n",
        "project_name = \"yohanTI\" #@param {type:\"string\"}\n",
        "initializer_word = \"illustration\" #@param {type:\"string\"}\n",
        "!python \"main.py\" \\\n",
        " --base {config} \\\n",
        " -t --no-test \\\n",
        " --actual_resume {model} \\\n",
        " --gpus 1 \\\n",
        " --data_root {dataset} \\\n",
        " --embedding_manager_ckpt {last_embed} \\\n",
        " --resume_from_checkpoint {last_checkpoint} \\\n",
        " -n {project_name} \\\n",
        " --init_word {initializer_word}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qcChN5YOAIgQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}